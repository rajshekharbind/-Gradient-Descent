# -Gradient-Descent

ğŸ“‰ Gradient Descent from Scratch with Animations

This project demonstrates Gradient Descent for Linear Regression implemented from scratch using NumPy, along with animated visualizations to build strong intuition.

We cover:

Gradient Descent fundamentals

Step-by-step parameter updates

Gradient Descent animation for both m and b

Gradient Descent animation for only b

Loss (cost) curve animation

Mathematical intuition behind updates

ğŸ“Œ Topics Covered
1ï¸âƒ£ Gradient Descent (Concept)

Gradient Descent is an iterative optimization algorithm used to minimize a loss function by updating parameters in the direction of the negative gradient.

2ï¸âƒ£ Gradient Descent â€“ Step by Step

At each epoch:

Compute predictions

Calculate loss (Mean Squared Error)

Compute gradients

Update parameters (m, b)

Repeat until convergence

3ï¸âƒ£ Gradient Descent Animation (Both m and b)

Visualizes the optimization path on the loss surface

Shows how slope (m) and intercept (b) change together

Helps understand why gradients point downhill

4ï¸âƒ£ Gradient Descent Animation (Only b)

Keeps slope (m) constant

Shows how changing intercept (b) affects loss

Ideal for beginners to understand 1D optimization

5ï¸âƒ£ Gradient Descent Code from Scratch

No ML libraries like sklearn.linear_model are used for training.
Everything is implemented manually using NumPy.

ğŸ§  Mathematical Intuition
Linear Model
ğ‘¦
^
=
ğ‘š
ğ‘¥
+
ğ‘
y
^
	â€‹

=mx+b
Loss Function (Mean Squared Error)
ğ½
(
ğ‘š
,
ğ‘
)
=
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
(
ğ‘¦
ğ‘–
âˆ’
(
ğ‘š
ğ‘¥
ğ‘–
+
ğ‘
)
)
2
J(m,b)=
n
1
	â€‹

i=1
âˆ‘
n
	â€‹

(y
i
	â€‹

âˆ’(mx
i
	â€‹

+b))
2
Gradients
âˆ‚
ğ½
âˆ‚
ğ‘š
=
âˆ’
2
ğ‘›
âˆ‘
ğ‘¥
ğ‘–
(
ğ‘¦
ğ‘–
âˆ’
ğ‘¦
^
ğ‘–
)
âˆ‚m
âˆ‚J
	â€‹

=âˆ’
n
2
	â€‹

âˆ‘x
i
	â€‹

(y
i
	â€‹

âˆ’
y
^
	â€‹

i
	â€‹

)
âˆ‚
ğ½
âˆ‚
ğ‘
=
âˆ’
2
ğ‘›
âˆ‘
(
ğ‘¦
ğ‘–
âˆ’
ğ‘¦
^
ğ‘–
)
âˆ‚b
âˆ‚J
	â€‹

=âˆ’
n
2
	â€‹

âˆ‘(y
i
	â€‹

âˆ’
y
^
	â€‹

i
	â€‹

)
Update Rules
ğ‘š
=
ğ‘š
âˆ’
ğ›¼
âˆ‚
ğ½
âˆ‚
ğ‘š
m=mâˆ’Î±
âˆ‚m
âˆ‚J
	â€‹

ğ‘
=
ğ‘
âˆ’
ğ›¼
âˆ‚
ğ½
âˆ‚
ğ‘
b=bâˆ’Î±
âˆ‚b
âˆ‚J
	â€‹


where Î± is the learning rate.

ğŸ¥ Visualizations Included

âœ” Regression line updating per epoch
âœ” Loss vs Epoch animation
âœ” Optimization path on contour plot
âœ” GIF animations using matplotlib.animation

ğŸ›  Dependencies & Installation
âœ… Required Libraries
pip install numpy matplotlib scikit-learn pillow ipython

ğŸ“¦ Dependency Purpose
Library	Usage
numpy	Numerical computation
matplotlib	Plotting & animation
scikit-learn	Dataset generation only
pillow	Save GIF animations
ipython	Animation display
â–¶ How to Run

Clone the repository

git clone <your-repo-url>
cd gradient-descent-animation


Open Jupyter Notebook

jupyter notebook


Run notebooks in order:

01_gradient_descent_from_scratch.ipynb

02_gradient_descent_only_b.ipynb

03_gradient_descent_m_and_b_animation.ipynb

04_loss_curve_animation.ipynb

ğŸ“‚ Project Structure
gradient-descent-animation/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ synthetic_data.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ gradient_descent_step_by_step.ipynb
â”‚   â”œâ”€â”€ gradient_descent_only_b.ipynb
â”‚   â”œâ”€â”€ gradient_descent_m_b_animation.ipynb
â”‚   â””â”€â”€ loss_curve_animation.ipynb
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ animation1.gif
â”‚   â”œâ”€â”€ animation2.gif
â”‚
â”œâ”€â”€ README.md

ğŸ¯ Learning Outcomes

After completing this project, you will:

Understand why gradient descent works

Visualize optimization in 1D and 2D

Implement gradient descent without ML libraries

Debug common animation & indexing errors

Build strong ML fundamentals

ğŸ“š References

Andrew Ng â€“ Machine Learning (Coursera)

Stanford CS229 Notes

Deep Learning Book â€“ Ian Goodfellow

Matplotlib Animation Docs

NumPy Documentation
